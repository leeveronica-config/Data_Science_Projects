- v1 is mostly an econometrics approach which is I gonna put aside for now.

- v2 works with linear models and a few approach to feature engineering. Started with linear regression and lags - from 7(week) to 31(month). Introducing "weekday" and "is_weekend" makes model only performing worse. "is_weekend" increases error from 23.94% to 32.09% and introducing "weekday" makes it even worse to 42.27%. Target encoding of "weekday" dropped error to 34.48%. Implementing Ridge gives 27.95%, Lasso 20.39%. XGboost rises up to 22.67%. 


- v2.1 here only the best results of v2 are kept. So no features are added, which bring us the baseline 23.94% with linear regression. No target encoding 
since there is not features introduced. 

- v3 - studying different "lags" intervals on the error rate.
	
	1,7 => lr: 13,44% ; Lasso: 13.46% 
	1,14 => lr: 13,21% ; Lasso: 13.06%
	1,21 => lr: 12,78% ; Lasso: 12.61%
	1,28 => lr: 13,41% ; Lasso: 12.88%
	1,42 => lr: 17,31% ; Lasso: 14.78%
	1,56 => lr: 21,36% ; Lasso: 18.48%
	
	7,14 => lr: ; Lasso:
	7,21 => lr: ; Lasso:
	7,28 => lr: ; Lasso:
	7,42 => lr: ; Lasso:
	7,56 => lr: ; Lasso:

	14,21 => lr: ; Lasso:
	14,28 => lr: ; Lasso:
	14,42 => lr: ; Lasso:
	14,56 => lr: ; Lasso:


Possible Features to generate and investigate:

Lags of time series

Window statistics:
 Max/min value of series in a window
 Average/median value in a window
 Window variance

Date and time features:
 Minute of an hour, hour of a day, day of the week, and so on
 Is this day a holiday? Maybe there is a special event? Represent that as a boolean feature

Target encoding
Forecasts from other models (note that we can lose the speed of prediction this way)/stacking